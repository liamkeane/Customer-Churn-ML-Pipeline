services:
  # MySQL database for telco db
  mysql:
    image: mysql:8.0
    container_name: mysql
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
      MYSQL_DATABASE: ${MYSQL_DATABASE}
      MYSQL_USER: ${MYSQL_USER}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD}
    ports:
      - "3307:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./sql/schema.sql:/docker-entrypoint-initdb.d/schema.sql
    networks:
      - telco_network

  # Postgres database for Airflow metadata
  postgres:
    container_name: postgres
    image: postgres:13
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER}"]
      interval: 5s
      retries: 5
    networks:
      - telco_network

  # Airflow webserver
  airflow-webserver:
    container_name: airflow-webserver
    build:
      context: .
      dockerfile: ./docker/airflow/Dockerfile
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    command: ["webserver"]
    networks:
      - telco_network

  # Airflow scheduler
  airflow-scheduler:
    container_name: airflow-scheduler
    build:
      context: .
      dockerfile: ./docker/airflow/Dockerfile
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: ["scheduler"]
    networks:
      - telco_network

  # https://medium.com/@opcfrance/setting-up-apache-airflow-with-docker-a-comprehensive-guide-with-examples-c041fed1c3f5

  # # Jupyter Lab for development and analysis
  # jupyter:
  #   image: jupyter/datascience-notebook:latest
  #   container_name: jupyter-lab
  #   restart: always
  #   environment:
  #     JUPYTER_ENABLE_LAB: "yes"
  #     JUPYTER_TOKEN: ${JUPYTER_TOKEN}
  #     GRANT_SUDO: "yes"
  #   ports:
  #     - "8888:8888"
  #   volumes:
  #     - ./notebooks:/home/jovyan/work
  #     - ./data:/home/jovyan/data
  #     - jupyter-user-settings:/home/jovyan/.jupyter
  #   networks:
  #     - telco_network
  #   depends_on:
  #     - mysql


  # MLflow for experiment tracking and model management
  # mlflow:
  #   build:
  #     context: .
  #     dockerfile: ./docker/mlflow/Dockerfile
  #   container_name: churn_mlflow
  #   environment:
  #     MLFLOW_BACKEND_STORE_URI: mysql+pymysql://airflow_user:airflow_password@mysql:3306/churn_analytics
  #     MLFLOW_DEFAULT_ARTIFACT_ROOT: /mlflow/artifacts
  #   ports:
  #     - "5000:5000"
  #   volumes:
  #     - mlflow_artifacts:/mlflow/artifacts
  #   depends_on:
  #     mysql:
  #       condition: service_healthy
  #   networks:
  #     - telco_network
  #   command: >
  #     mlflow server
  #     --backend-store-uri mysql+pymysql://airflow_user:airflow_password@mysql:3306/churn_analytics
  #     --default-artifact-root /mlflow/artifacts
  #     --host 0.0.0.0
  #     --port 5000

volumes:
  mysql_data:
  postgres_data:
  # mlflow_artifacts:
  # jupyter-user-settings:

networks:
  telco_network:
    driver: bridge